{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efe75b94",
   "metadata": {},
   "source": [
    "# Compute Mode Connectivity Graph & Merge Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b5478f",
   "metadata": {},
   "source": [
    "Notes\n",
    "\n",
    "- The full pipeline includes:\n",
    "    - train PINN models using 100 different random seeds\n",
    "    - setup MC runs using `setup_MC_runs.ipynb` (this creates a file used by `train_eval_pinn_multi.py`)\n",
    "    - submit MC runs using `train_eval_pinn_multi.py`\n",
    "    - process MC runs and construct graphs using this notebook\n",
    "\n",
    "<br>\n",
    "\n",
    "- Results are strored in the following locations\n",
    "    - individual checkpoints: \n",
    "    ```\n",
    "    /global/cfs/cdirs/m636/geniesse/projects/characterizing-pinns-failure-modes/pbc_examples/checkpoints\n",
    "    ```\n",
    "    - curve checkpoints:\n",
    "    ```\n",
    "    /global/cfs/cdirs/m636/geniesse/projects/dnn-mode-connectivity/checkpoints_global\n",
    "    ```\n",
    "\n",
    "<br>\n",
    "\n",
    "- Requires installing the following packages:\n",
    "\n",
    "    - https://github.com/mrzv/nesoi\n",
    "    - https://ripser.scikit-tda.org/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0808903a-208e-40ec-8f74-b7fe57c589a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/mrzv/nesoi.git "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2a0cb9f-9c6b-42a0-b970-985ed20bbbe9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install Ripser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eea5c8",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d337d61d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c893e355",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import scipy \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e7ec21",
   "metadata": {},
   "source": [
    "# Compute pairwise-connected graph\n",
    "\n",
    "1. Process model pair information\n",
    "2. Construct graph and add mode connected point between each edge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3eeedd",
   "metadata": {},
   "source": [
    "## Process model pair information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004f1365-1526-49ab-82f2-868bcdeeff91",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here is an example checkpoint folder for a single curve. Note, the folder name includes the name of the two models connected by the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fe9857",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls checkpoints_global/PINN_convection_beta_1.0_lr_1.0_seed_001_PINN_convection_beta_1.0_lr_1.0_seed_002"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818f4ce2-f691-4e05-b9e3-b365d68b6c5e",
   "metadata": {},
   "source": [
    "Here is a quick overview of the options that can be changed below:\n",
    "- `beta = {1.0, 50.0}    # PINN wave speed coefficient`\n",
    "- `eval_epoch = {0, 50}  # how long the curve was trained for (0: linear MC, 50: nonlinear MC)`\n",
    "- `keep_nodes = [2â€¦100]  # how many models to include in the graph` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ba92e1-98e9-4f69-bc45-c8808d65e056",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# configure options here\n",
    "beta = 50.0      # PINN wave speed coefficient\n",
    "eval_epoch = 0   # how long the curve was trained for (0: linear MC, 50: nonlinear MC)\n",
    "keep_nodes = 5  # how many models to include in the graph (max 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ac653e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load pairs \n",
    "pairs_file = f\"PINN_convection_beta_{beta}_lr_1.0_n_seeds_100_pairs.csv\"\n",
    "df_pairs = pd.read_csv(pairs_file)\n",
    "\n",
    "\n",
    "# assign curve name \n",
    "df_pairs = df_pairs.assign(\n",
    "    curve_name=[f\"{_.init_start}_{_.init_end}\".replace(\".pt\",\"\") for __,_ in df_pairs.iterrows()]\n",
    ")\n",
    "\n",
    "# assign result folder \n",
    "df_pairs = df_pairs.assign(\n",
    "    result_file=[f\"checkpoints_global/{_}/checkpoint-{eval_epoch}_curve.npz\" for _ in df_pairs.curve_name]\n",
    ")\n",
    "\n",
    "# assign result folder \n",
    "df_pairs = df_pairs.assign(\n",
    "    checkpoint_file=[f\"checkpoints_global/{_}/checkpoint-{eval_epoch}.pt\" for _ in df_pairs.curve_name]\n",
    ")\n",
    "\n",
    "# assign source,target values\n",
    "df_pairs = df_pairs.assign(\n",
    "    source=[int(_.split(\"_seed_\")[-1].split(\".pt\")[0]) for _ in df_pairs.init_start],\n",
    "    target=[int(_.split(\"_seed_\")[-1].split(\".pt\")[0]) for _ in df_pairs.init_end]\n",
    ")\n",
    "\n",
    "# assign edge tuples\n",
    "df_pairs = df_pairs.assign(\n",
    "    edge=[_ for _ in zip(df_pairs.source.values, df_pairs.target.values)]\n",
    ")\n",
    "\n",
    "# limit to the first n nodes\n",
    "df_pairs = df_pairs[df_pairs.source.le(keep_nodes) & df_pairs.target.le(keep_nodes)]\n",
    "df_pairs = df_pairs.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# show df\n",
    "df_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac73588",
   "metadata": {},
   "source": [
    "## Construct graph and add mode connected point between each edge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35284a7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize graph based on edges\n",
    "edges = df_pairs.edge.to_list()\n",
    "G1 = nx.Graph(edges)\n",
    "\n",
    "# construct new graph (with extra nodes)\n",
    "G2 = nx.Graph()\n",
    "\n",
    "# loop over edges \n",
    "for index, df_pair in df_pairs.iterrows():\n",
    "\n",
    "    # extract curve metrics for the edge pair\n",
    "    with np.load(df_pair.result_file) as result:\n",
    "        curve_losses = result['tr_error_u_rel']\n",
    "        # curve_losses = result['tr_loss']\n",
    "        # mc = float(result['mc_metric'])\n",
    "    \n",
    "    # assign node id to the new node\n",
    "    curve_node = len(G1) + index + 1\n",
    "    source_node = df_pair.source\n",
    "    target_node = df_pair.target\n",
    "    \n",
    "    # update G1\n",
    "    G1.nodes[source_node]['loss'] = curve_losses[0]\n",
    "    G1.nodes[target_node]['loss'] = curve_losses[-1]\n",
    "\n",
    "    # assign node data\n",
    "    G2.add_node(source_node, loss=curve_losses[0])\n",
    "    G2.add_node(curve_node, loss=curve_losses[np.argmax(np.abs(curve_losses))])\n",
    "    G2.add_node(target_node, loss=curve_losses[-1])\n",
    "\n",
    "    # add new path going through the curve node\n",
    "    nx.add_path(G2, [source_node, curve_node, target_node])\n",
    "    print(f\"[+] ({source_node}, {curve_node:2d}, {target_node}) => ({G2.nodes[source_node]['loss']:.6f}, {G2.nodes[curve_node]['loss']:.6f}, {G2.nodes[target_node]['loss']:.6f})\")\n",
    "\n",
    "# show some things\n",
    "print(f\"{G1.number_of_nodes()=}\")\n",
    "print(f\"{G1.number_of_edges()=}\")\n",
    "print(f\"{G2.number_of_nodes()=}\")\n",
    "print(f\"{G2.number_of_edges()=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63bdec4",
   "metadata": {},
   "source": [
    "## Convert graphs to format for `nesoi`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e50807a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save graph edges as np array\n",
    "edges = [_[:2] for _ in nx.to_edgelist(G2, nodelist=None)]\n",
    "edges = np.array(edges) \n",
    "edges = edges - 1 # re-index for nesoi\n",
    "\n",
    "# save loss values\n",
    "loss = np.array([G2.nodes[_]['loss'] for _ in sorted(G2.nodes)])\n",
    "\n",
    "# save things (optional)\n",
    "# save_as = f\"{pairs_file.replace('.csv','')}_eval_epoch_{eval_epoch}_keep_nodes_{keep_nodes}.npz\"\n",
    "# np.savez(save_as, edges=edges, loss=loss)\n",
    "# print(save_as)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6a1d0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Compute Merge Tree (using `nesoi`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564573c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import nesoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0700ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# construct a new graph for nesoi (not sure this is necessary... can we just use G2 from above?)\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(list(range(len(loss))))\n",
    "G.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abda17de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tree = nesoi.TMT_float(len(loss), False)\n",
    "\n",
    "for i,v in enumerate(loss):\n",
    "    tree.add(i,v)\n",
    "\n",
    "for e in edges:\n",
    "    # tree.merge(e[0]-1, e[1]-1)\n",
    "    tree.merge(e[0], e[1])\n",
    "\n",
    "tree.repair()\n",
    "\n",
    "# Output persistence diagram\n",
    "for (u,s,v) in tree.traverse_persistence():\n",
    "    print(u,s,v, loss[u], loss[s])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cd7253",
   "metadata": {},
   "source": [
    "## Compute full tree (includes degree-2 nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68de6794",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate conventional tree\n",
    "from collections import defaultdict\n",
    "paths = defaultdict(set)\n",
    "\n",
    "for u in range(len(loss)):\n",
    "    (s,v) = tree[u]\n",
    "    if u == v: continue\n",
    "    paths[v].add(s)\n",
    "    if u != s:\n",
    "        paths[u].add(s)\n",
    "\n",
    "for u in paths.keys():\n",
    "    paths[u] = list(paths[u])\n",
    "    paths[u].sort(key = lambda x: loss[x])\n",
    "\n",
    "path_edges = []\n",
    "for u in paths.keys():\n",
    "    p = paths[u]\n",
    "    print(u,p[0])\n",
    "    path_edges.append((u, p[0]))\n",
    "\n",
    "    for i in range(len(paths[u]) - 1):\n",
    "        print(p[i], p[i+1])\n",
    "        path_edges.append((p[i], p[i+1]))\n",
    "\n",
    "        \n",
    "\n",
    "T_full = nx.Graph()\n",
    "T_full.add_nodes_from(list(range(len(loss))))\n",
    "T_full.add_edges_from(path_edges)\n",
    "\n",
    "T_full.remove_nodes_from([n for n in T_full if not len(list(T_full.neighbors(n)))])\n",
    "\n",
    "height_full = {n: loss[n] for n in T_full}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dd4828",
   "metadata": {},
   "source": [
    "## Compute condensed tree (no degree-2 nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5934851d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate conventional tree\n",
    "from collections import defaultdict\n",
    "paths = defaultdict(list)\n",
    "\n",
    "for (u,s,v) in tree.traverse_persistence():\n",
    "    if u == v: continue\n",
    "    paths[v].append(s)\n",
    "    paths[u].append(s)\n",
    "\n",
    "for u in paths.keys():\n",
    "    paths[u].sort(key = lambda x: loss[x])\n",
    "\n",
    "path_edges = []\n",
    "for u in paths.keys():\n",
    "    p = paths[u]\n",
    "    print(u,p[0])\n",
    "    path_edges.append((u, p[0]))\n",
    "\n",
    "    for i in range(len(paths[u]) - 1):\n",
    "        print(p[i], p[i+1])\n",
    "        path_edges.append((p[i], p[i+1]))\n",
    "\n",
    "        \n",
    "import networkx as nx\n",
    "\n",
    "T = nx.Graph()\n",
    "T.add_nodes_from(list(range(len(loss))))\n",
    "T.add_edges_from(path_edges)\n",
    "T.remove_nodes_from([n for n in T if not len(list(T.neighbors(n)))])\n",
    "\n",
    "\n",
    "### DEBUGGING \n",
    "# connect max in the tree with max in full tree\n",
    "# T.add_edges_from([(13,11)])\n",
    "nodes = list(T.nodes)\n",
    "nodes.sort(key=lambda x: loss[x])\n",
    "max_node = nodes[-1]\n",
    "\n",
    "full_nodes = list(T_full.nodes)\n",
    "full_nodes.sort(key=lambda x: loss[x])\n",
    "max_node_full = full_nodes[-1]\n",
    "print()\n",
    "print(max_node, max_node_full)\n",
    "T.add_edges_from([(max_node,max_node_full)])\n",
    "\n",
    "height = {n: loss[n] for n in T}\n",
    "# height = {n: loss[n] for n in T if len(T[n])} \n",
    "# height = {n: loss[n] for n in T if len(list(T.neighbors(n)))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c40275d",
   "metadata": {},
   "source": [
    "## Save intermediates (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec289692",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open(\"merge_tree.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(T, f)\n",
    "    \n",
    "# with open(\"merge_tree_full.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(T_full, f)\n",
    "    \n",
    "# with open(\"height.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(height, f) \n",
    "    \n",
    "# with open(\"height_full.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(height_full, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5145a670",
   "metadata": {},
   "source": [
    "## Draw the trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709af06c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "nx.draw_kamada_kawai(T_full, node_color = list(height_full.values()), with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2993902b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "nx.draw_kamada_kawai(T, node_color = list(height.values()), with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385c4eba",
   "metadata": {},
   "source": [
    "## Draw the trees (using DMT_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d9200d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# borrowed from https://github.com/trneedham/Decorated-Merge-Trees\n",
    "from DMT_tools import mergeTree_pos\n",
    "def draw_merge_tree(G,height,axes=False, ax=None, **kwargs):\n",
    "    # Input: merge tree as G, height\n",
    "    # Output: draws the merge tree with correct node heights\n",
    "    pos = mergeTree_pos(G,height)\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    nx.draw_networkx(G, pos=pos, ax=ax, with_labels=True, **kwargs)\n",
    "    if axes:\n",
    "        ax.tick_params(left=True, bottom=False, labelleft=True, labelbottom=False)\n",
    "    return\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "draw_merge_tree(T_full, height_full, ax=ax, node_size=500, width=10, node_color=\"indianred\", edge_color=\"indianred\")\n",
    "draw_merge_tree(T, height, ax=ax, node_size=1000, width=10, node_color=\"cadetblue\", edge_color=\"cadetblue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d055e0",
   "metadata": {},
   "source": [
    "# Save Merge Tree files (for 1D profiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654e2ccc",
   "metadata": {},
   "source": [
    "The minimum Format:\n",
    "\n",
    "    Merge Tree Edges\n",
    "    \"SegmentationId\",\"upNodeId\",\"downNodeId\"\n",
    "\n",
    "    Merge Tree Nodes\n",
    "    \"NodeId\",\"Scalar\",\"CriticalType\"\n",
    "\n",
    "    Merge Tree Segmentations\n",
    "    \"Loss\",\"SegmentationId\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a009e091",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aece0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_critical_type(T, n):\n",
    "    # 0=minima, 1=saddle, 3=root\n",
    "    deg = len(list(T.neighbors(n))) # use the number of neighbors as the critical point?\n",
    "    if max(loss) == loss[n]:\n",
    "        return 3\n",
    "    if deg == 1:\n",
    "        return 0\n",
    "    if deg == 3:\n",
    "        return 1\n",
    "    return -1\n",
    "\n",
    "sys.setrecursionlimit(5050)\n",
    "def find_nearest_critical_point(G, n, verbose=0):\n",
    "    if verbose > 0: \n",
    "        print(f\"{n=}\")\n",
    "    \n",
    "    # compute neighbors\n",
    "    nbrs = list(G.neighbors(n))\n",
    "\n",
    "    # return if critical point \n",
    "    if len(nbrs) != 2:\n",
    "        if verbose > 0: \n",
    "            print(f\"Found critical point!!! ({n=})\")\n",
    "        return n\n",
    "    \n",
    "    # TODO: sort neighbors by loss\n",
    "    if verbose > 1: \n",
    "        print(n, nbrs)\n",
    "    nbrs.sort(key = lambda x: loss[x])\n",
    "    if verbose > 1: \n",
    "        print(n, nbrs)\n",
    "    \n",
    "    # traverse neighbors until critical point is found\n",
    "    for nbr in nbrs:\n",
    "        if verbose > 0: \n",
    "            print(f\"\\t{nbr=}\")\n",
    "        return find_nearest_critical_point(G, nbr)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e17cc92",
   "metadata": {},
   "source": [
    "## Initialize data frames for saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dfcfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mt_edges = pd.DataFrame(columns=[\"SegmentationId\",\"upNodeId\",\"downNodeId\"])\n",
    "df_mt_nodes = pd.DataFrame(columns=[\"NodeId\",\"Scalar\",\"CriticalType\"])\n",
    "df_mt_seg = pd.DataFrame(columns=[\"Loss\",\"SegmentationId\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3f9282",
   "metadata": {},
   "source": [
    "## Sort edges in the merge tree by loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035a964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_edges_sorted = [list(_) for _ in T.edges()]\n",
    "for T_edge in T_edges_sorted:\n",
    "    print(T_edge)\n",
    "    T_edge.sort(key = lambda x: loss[x])\n",
    "    print(T_edge)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a0a8c9",
   "metadata": {},
   "source": [
    "## Store merge tree edge information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cecb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mt_edges = pd.DataFrame(columns=[\"SegmentationId\",\"upNodeId\",\"downNodeId\"])\n",
    "df_mt_edges = df_mt_edges.assign(SegmentationId = [i for i,_ in enumerate(T_edges_sorted)])    \n",
    "df_mt_edges = df_mt_edges.assign(upNodeId = [_[1] for i,_ in enumerate(T_edges_sorted)])\n",
    "df_mt_edges = df_mt_edges.assign(downNodeId = [_[0] for i,_ in enumerate(T_edges_sorted)])\n",
    "df_mt_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f5ff9b",
   "metadata": {},
   "source": [
    "## Store merge tree node information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29ef0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mt_nodes = pd.DataFrame(columns=[\"NodeId\",\"Scalar\",\"CriticalType\"])\n",
    "df_mt_nodes = df_mt_nodes.assign(NodeId = list(set(np.ravel(T.edges()))))    \n",
    "df_mt_nodes = df_mt_nodes.assign(Scalar = [loss[_] for _ in df_mt_nodes.NodeId])\n",
    "df_mt_nodes = df_mt_nodes.assign(CriticalType = [\n",
    "    get_critical_type(T, _)\n",
    "    for _ in df_mt_nodes.NodeId\n",
    "])\n",
    "df_mt_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169df483",
   "metadata": {},
   "source": [
    "## Store merge tree segmentation information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2e9e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mt_seg = pd.DataFrame(columns=[\"Loss\",\"SegmentationId\"])\n",
    "df_mt_seg = df_mt_seg.assign(Loss = loss)\n",
    "\n",
    "### find the nearest critical point in T_full\n",
    "### ... map the critical point to segmentationId based on down node\n",
    "for node_id in range(len(loss)):\n",
    "    \n",
    "    print(node_id)\n",
    "    \n",
    "    # find the nearest down node\n",
    "    down_node_id = find_nearest_critical_point(T_full, node_id)\n",
    "    \n",
    "    # find the segmentation id for the edge\n",
    "    if down_node_id not in df_mt_edges.downNodeId.values:\n",
    "        # this should only happen for root node\n",
    "        print(f\"Found possible root node ({node_id=}, {down_node_id=}) ... using SegmentationId based on upNodeId ({seg_id=})\")\n",
    "        seg_id = df_mt_edges[df_mt_edges.upNodeId.eq(down_node_id)].SegmentationId.values[0]\n",
    "    else:\n",
    "        seg_id = df_mt_edges[df_mt_edges.downNodeId.eq(down_node_id)].SegmentationId.values[0]\n",
    "\n",
    "    # display results\n",
    "    # print(f\"{node_id=}, {down_node_id=}, {seg_id=}\")\n",
    "    \n",
    "    # update segmentation \n",
    "    df_mt_seg.at[node_id, 'SegmentationId'] = seg_id\n",
    "    \n",
    "# show df_mt_seg\n",
    "df_mt_seg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc49f17",
   "metadata": {},
   "source": [
    "## Re-compute the merge tree edge information (after using it for the segmentation)\n",
    "\n",
    "a bit hacky... i know"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a145bff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Re Number edges AFTER using it for segmentation\n",
    "df_mt_edges = pd.DataFrame(columns=[\"SegmentationId\",\"upNodeId\",\"downNodeId\"])\n",
    "df_mt_edges = df_mt_edges.assign(SegmentationId = [i for i,_ in enumerate(T_edges_sorted)])    \n",
    "df_mt_edges = df_mt_edges.assign(upNodeId = [_[1] for i,_ in enumerate(T_edges_sorted)])\n",
    "df_mt_edges = df_mt_edges.assign(downNodeId = [_[0] for i,_ in enumerate(T_edges_sorted)])\n",
    "\n",
    "T_nodes = list(np.sort(np.unique(np.ravel(T_edges_sorted))))\n",
    "print(T_nodes)\n",
    "\n",
    "df_mt_edges = df_mt_edges.assign(upNodeIdDataId = df_mt_edges.upNodeId)\n",
    "df_mt_edges = df_mt_edges.assign(downNodeIdDataId = df_mt_edges.downNodeId)\n",
    "\n",
    "df_mt_edges = df_mt_edges.assign(upNodeId = df_mt_edges.upNodeIdDataId.apply(T_nodes.index))\n",
    "df_mt_edges = df_mt_edges.assign(downNodeId = df_mt_edges.downNodeIdDataId.apply(T_nodes.index))\n",
    "\n",
    "\n",
    "df_mt_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc2c101",
   "metadata": {},
   "source": [
    "## Save the merge tree files (consumed by the topological profile code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9d35e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = f\"PINN_convection_beta_{beta}_lr_1.0_n_seeds_100_pairs_eval_epoch_{eval_epoch}_keep_nodes_{keep_nodes}.npz\"\n",
    "\n",
    "mt_nodes_file = file_name.replace(\".npz\", \"_MergeTree.csv\")\n",
    "mt_edges_file = file_name.replace(\".npz\", \"_MergeTree_edge.csv\")\n",
    "mt_seg_file = file_name.replace(\".npz\", \"_MergeTree_segmentation.csv\")\n",
    "\n",
    "df_mt_nodes.to_csv(mt_nodes_file, index=None)\n",
    "df_mt_edges.to_csv(mt_edges_file, index=None)\n",
    "df_mt_seg.to_csv(mt_seg_file, index=None)\n",
    "\n",
    "print(f\"[+] {mt_nodes_file}\")\n",
    "print(f\"[+] {mt_edges_file}\")\n",
    "print(f\"[+] {mt_seg_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5694ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinns",
   "language": "python",
   "name": "pinns"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
